{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, Normalizer\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slash = \"/\" if \"Linux\" in platform.platform() else \"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_link = os.getcwd() + f'{slash}Dataset_TBI.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_excel(data_link, index_col=0)\n",
    "print(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['GOS-E (t2)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tbi = df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_value1 = [1, 2, 3 ,4]\n",
    "old_value2 = [5, 6, 7, 8]\n",
    "new_value1 = 0\n",
    "new_value2 = 1\n",
    "for i, j in zip(old_value1, old_value2):\n",
    "    df_tbi['GOS-E (t2)'] = df_tbi['GOS-E (t2)'].replace(i, new_value1)\n",
    "    df_tbi['GOS-E (t2)'] = df_tbi['GOS-E (t2)'].replace(j, new_value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tbi['GOS-E (t2)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Diagnosis at Discharge (t2)', 'GOS-E (t1)', 'CRS-R (t2)', 'RLAS (t2)', 'DRS (t2)', 'ERBI A (t2)', 'ERBI B (t2)']\n",
    "for i in range(7):\n",
    "    df_tbi = df_tbi.drop(drop_list[i], axis = 1)\n",
    "# df_tbi = df_tbi.drop('GOS-E (t1)', axis = 1)\n",
    "# df_tbi = df_tbi.drop('Diagnosis at Discharge (t2)', axis = 1)\n",
    "# df_tbi = df_tbi.drop('Diagnosis at Discharge (t2)', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_tbi['GOS-E (t2)']\n",
    "features = df_tbi.drop('GOS-E (t2)', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder1 = OneHotEncoder()\n",
    "encoder2 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tbi['Marshall (t0)'] = encoder2.fit_transform(df_tbi['Marshall (t0)'])\n",
    "df_tbi['Sex'] = encoder2.fit_transform(df_tbi['Sex'])\n",
    "df_tbi['Entry Diagnosis (t0)'] = encoder2.fit_transform(df_tbi['Entry Diagnosis (t0)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = scaler.fit_transform(features)\n",
    "features_scaled = pd.DataFrame(features_array)\n",
    "features_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo đối tượng KNNImputer với tham số n_neighbors=25\n",
    "knn_imputer = KNNImputer(n_neighbors=25)\n",
    "df = knn_imputer.fit_transform(features_scaled)\n",
    "df = pd.DataFrame(features_scaled)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mrmr\n",
    "# from mrmr import mrmr_classif\n",
    "import pymrmr\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrmr(X, y):\n",
    "    num_features = X.shape[1]\n",
    "    selected_features = []\n",
    "    mrmr_scores = []\n",
    "\n",
    "    for _ in range(num_features):\n",
    "        scores = []\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            if feature not in selected_features:\n",
    "                relevance = mutual_info_classif(X.iloc[:, feature].astype(float).values.reshape(-1, 1), y.astype(float))\n",
    "                redundancy = sum([mutual_info_score(X.iloc[:, feature].astype(float), X.iloc[:, selected_feature].astype(float)) for selected_feature in selected_features])\n",
    "                mrmr_score = relevance - (1 / len(selected_features) if len(selected_features) > 0 else 0) * redundancy\n",
    "                scores.append((feature, mrmr_score))\n",
    "                mrmr_scores.append(mrmr_score)\n",
    "\n",
    "        # scores = sorted(scores, key = lambda x: x[1], reverse = True)\n",
    "        # selected_feature = scores[0][0]\n",
    "        # selected_features.append(selected_feature)\n",
    "\n",
    "    # return selected_features, mrmr_scores, scores\n",
    "    return scores\n",
    "    \n",
    "# Example usage with your data\n",
    "scores = mrmr(X= df, y=target)\n",
    "# selected_features, mrmr_scores, scores = mrmr(X= df_new_tbi, y=df_new_tbi_target)\n",
    "\n",
    "print(scores)\n",
    "# print(\"Selected Features:\", selected_features)\n",
    "# for feature, score in zip(selected_features, mrmr_scores):\n",
    "#     print(f\"Feature {feature}: MRMR Score = {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = {'Age':0.10352305, 'Sex':0.0072446, 'Marshall (t0)': 0.05729801, 'Entry Diagnosis (t0)':0.16496111, 'CRS-R (t1)':0.18392238,\n",
    "#        'RLAS (t1)':0.14162986, 'DRS (t1)':0.14655443, 'ERBI A (t1)':0.0766897, 'ERBI B (t1)':0.03195924}\n",
    "\n",
    "# l_sorted = sorted(l.items(), key=lambda x:x[1], reverse=True)\n",
    "# print(l_sorted)\n",
    "# l_sorted_dict = {'CRS-R (t1)': 0.18392238, 'Entry Diagnosis (t0)': 0.16496111, 'DRS (t1)': 0.14655443, \n",
    "#             'RLAS (t1)': 0.14162986, 'Age': 0.10352305, 'ERBI A (t1)': 0.0766897, 'Marshall (t0)': 0.05729801, \n",
    "#             'ERBI B (t1)': 0.03195924, 'Sex': 0.0072446}\n",
    "\n",
    "# names = list(l_sorted_dict.keys())\n",
    "# values = list(l_sorted_dict.values())\n",
    "# plt.figure(figsize=(14,6))\n",
    "# plt.bar(range(len(l_sorted_dict)), values, tick_label=names)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RLAS (t1) Age DRS (t1) CRS-R (t1)\tEntry Diagnosis (t0) ERBI A (t1) Marshall (t0) ERBI B (t1) Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = df[['CRS-R (t1)', 'Entry Diagnosis (t0)', 'DRS (t1)','RLAS (t1)', 'Age']]\n",
    "X_selected\n",
    "y = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "loocv = LeaveOneOut()\n",
    "# Define custom scoring function\n",
    "def custom_scoring(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'sensitivity': recall,\n",
    "        'specificity': specificity\n",
    "    }\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Define parameter grids for each classifier\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10, 100], 'penalty': ['l2']},\n",
    "    'SVM': {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf']},\n",
    "    'Decision Tree': {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "    'KNN': {'n_neighbors': [3, 5, 7, 9]},\n",
    "    'Gaussian Naive Bayes': {},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for each classifier\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    print(\"Classifier:\", classifier_name)\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=classifier, param_grid=param_grids[classifier_name], \n",
    "                               scoring=make_scorer(custom_scoring), cv=loocv, refit='accuracy', verbose=1)\n",
    "    \n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    \n",
    "    # Get the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate the best model\n",
    "    y_pred_best = cross_val_predict(best_model, X_train, y_train, cv=loocv)\n",
    "    scores_best = custom_scoring(y_train, y_pred_best)\n",
    "    print(\"Best Model Scores:\", scores_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 10-fold cross-validation\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# Define custom scoring function\n",
    "def custom_scoring(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'sensitivity': recall,\n",
    "        'specificity': specificity\n",
    "    }\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Define parameter grids for each classifier\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10, 100], 'penalty': ['l2']},\n",
    "    'SVM': {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf']},\n",
    "    'Decision Tree': {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "    'KNN': {'n_neighbors': [3, 5, 7, 9]},\n",
    "    'Gaussian Naive Bayes': {},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for each classifier\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    print(\"Classifier:\", classifier_name)\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=classifier, param_grid=param_grids[classifier_name], \n",
    "                               scoring=make_scorer(custom_scoring), cv=k_fold, refit='accuracy', verbose=1)\n",
    "    \n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    \n",
    "    # Get the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate the best model\n",
    "    y_pred_best = cross_val_predict(best_model, X_train, y_train, cv = k_fold)\n",
    "    scores_best = custom_scoring(y_train, y_pred_best)\n",
    "    print(\"Best Model Scores:\", scores_best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
