{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "079e5e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, f1_score, confusion_matrix, precision_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "print(os.getcwd())\n",
    "\n",
    "data_link = os.getcwd() + \"\\\\TBI_data_2.csv\"\n",
    "print(data_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b289d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     record_id  age_at_record  sex  tbi_cli_reason  tbi_cli_time_acci_hos  \\\n",
      "0          109           52.0  0.0             1.0                    2.0   \n",
      "1          110           75.0  0.0             7.0                    NaN   \n",
      "2          112           37.0  0.0             1.0                    NaN   \n",
      "3          113           66.0  0.0             1.0                    NaN   \n",
      "4          114           20.0  0.0             1.0                    4.0   \n",
      "..         ...            ...  ...             ...                    ...   \n",
      "255        391           22.0  0.0             7.0                    2.0   \n",
      "256        392           40.0  0.0             1.0                    2.0   \n",
      "257        393           84.0  0.0             1.0                 1440.0   \n",
      "258        394           44.0  0.0             1.0                    5.0   \n",
      "259        395           43.0  0.0             1.0                    3.0   \n",
      "\n",
      "     tbi_cli_pulse  tbi_cli_temp  tbi_cli_blood_pressure  \\\n",
      "0             70.0          36.5                     2.0   \n",
      "1             77.0          36.8                     2.0   \n",
      "2             67.0          36.6                     2.0   \n",
      "3             75.0          36.5                     2.0   \n",
      "4             85.0          36.8                     2.0   \n",
      "..             ...           ...                     ...   \n",
      "255            NaN           NaN                     NaN   \n",
      "256            NaN           NaN                     NaN   \n",
      "257            NaN           NaN                     NaN   \n",
      "258            NaN           NaN                     NaN   \n",
      "259            NaN           NaN                     NaN   \n",
      "\n",
      "     tbi_cli_breathing_rate  tbi_cli_glasgow  ...  ast_v2  alt_v2  \\\n",
      "0                      19.0               13  ...     1.0     1.0   \n",
      "1                      22.0               13  ...     0.0     0.0   \n",
      "2                      18.0               15  ...     0.0     0.0   \n",
      "3                      19.0               14  ...     1.0     0.0   \n",
      "4                      18.0               15  ...     0.0     0.0   \n",
      "..                      ...              ...  ...     ...     ...   \n",
      "255                     NaN               15  ...     0.0     0.0   \n",
      "256                     NaN                8  ...     0.0     0.0   \n",
      "257                     NaN               13  ...     0.0     0.0   \n",
      "258                     NaN               14  ...     1.0     1.0   \n",
      "259                     NaN               15  ...     1.0     1.0   \n",
      "\n",
      "     d_2_protein  albumin_v2  ure_v2  creatinin_v2  prothrombin_v2  d_3_aptt  \\\n",
      "0            0.0         1.0     0.0           0.0             0.0       0.0   \n",
      "1            0.0         1.0     0.0           0.0             0.0       0.0   \n",
      "2            0.0         NaN     0.0           0.0             0.0       0.0   \n",
      "3            1.0         0.0     0.0           0.0             0.0       0.0   \n",
      "4            0.0         NaN     0.0           0.0             0.0       0.0   \n",
      "..           ...         ...     ...           ...             ...       ...   \n",
      "255          NaN         0.0     0.0           1.0             0.0       1.0   \n",
      "256          NaN         0.0     0.0           0.0             0.0       1.0   \n",
      "257          NaN         NaN     0.0           1.0             0.0       1.0   \n",
      "258          NaN         0.0     0.0           0.0             0.0       0.0   \n",
      "259          NaN         0.0     0.0           0.0             0.0       0.0   \n",
      "\n",
      "     d_4_dtim  d_kl_tl  \n",
      "0         1.0        2  \n",
      "1         NaN        2  \n",
      "2         1.0        2  \n",
      "3         NaN        3  \n",
      "4         NaN        2  \n",
      "..        ...      ...  \n",
      "255       1.0        2  \n",
      "256       1.0        3  \n",
      "257       1.0        3  \n",
      "258       1.0        2  \n",
      "259       1.0        2  \n",
      "\n",
      "[260 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "df_tbi = pd.read_csv(data_link, delimiter = \",\")\n",
    "print(df_tbi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83738ae6",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b7b94",
   "metadata": {},
   "source": [
    "### 4 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b61e4a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tbi_cli_glasgow</th>\n",
       "      <th>ethanol</th>\n",
       "      <th>tbi_cli_breathing_rate</th>\n",
       "      <th>tbi_ct_subdural_hematoma_position_proprotion</th>\n",
       "      <th>tbi_ct_subdural_hematoma_position_value</th>\n",
       "      <th>tbi_ct_rotterdam</th>\n",
       "      <th>tbi_ct_epidural_hematoma_value</th>\n",
       "      <th>tbi_ct_subdural_hematoma_thick</th>\n",
       "      <th>tbi_ct_cerebral_contusion_volume</th>\n",
       "      <th>age_at_record</th>\n",
       "      <th>tbi_cli_time_acci_hos</th>\n",
       "      <th>tbi_ct_skull_risk</th>\n",
       "      <th>tbi_ct_epidural_hematoma_volume</th>\n",
       "      <th>tbi_cli_pulse</th>\n",
       "      <th>tbi_ct_skull_fracture_characteristic</th>\n",
       "      <th>tbi_ct_subarachnoid_characteristic</th>\n",
       "      <th>tbi_cli_hypertension</th>\n",
       "      <th>tbi_cli_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>237.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tbi_cli_glasgow  ethanol  tbi_cli_breathing_rate  \\\n",
       "0               13    237.9                    19.0   \n",
       "1               13      0.0                    22.0   \n",
       "2               15      NaN                    18.0   \n",
       "3               14      NaN                    19.0   \n",
       "4               15      0.0                    18.0   \n",
       "\n",
       "   tbi_ct_subdural_hematoma_position_proprotion  \\\n",
       "0                                           1.0   \n",
       "1                                           1.0   \n",
       "2                                           NaN   \n",
       "3                                           2.0   \n",
       "4                                           NaN   \n",
       "\n",
       "   tbi_ct_subdural_hematoma_position_value  tbi_ct_rotterdam  \\\n",
       "0                                     70.0               2.0   \n",
       "1                                     35.0               NaN   \n",
       "2                                      NaN               2.0   \n",
       "3                                     60.0               2.0   \n",
       "4                                      NaN               1.0   \n",
       "\n",
       "   tbi_ct_epidural_hematoma_value  tbi_ct_subdural_hematoma_thick  \\\n",
       "0                             NaN                             7.0   \n",
       "1                             NaN                            23.0   \n",
       "2                             NaN                             NaN   \n",
       "3                             NaN                             6.0   \n",
       "4                             NaN                             NaN   \n",
       "\n",
       "   tbi_ct_cerebral_contusion_volume  age_at_record  tbi_cli_time_acci_hos  \\\n",
       "0                               NaN           52.0                    2.0   \n",
       "1                               NaN           75.0                    NaN   \n",
       "2                               2.0           37.0                    NaN   \n",
       "3                               NaN           66.0                    NaN   \n",
       "4                               NaN           20.0                    4.0   \n",
       "\n",
       "   tbi_ct_skull_risk  tbi_ct_epidural_hematoma_volume  tbi_cli_pulse  \\\n",
       "0                2.0                              NaN           70.0   \n",
       "1                2.0                              NaN           77.0   \n",
       "2                2.0                              NaN           67.0   \n",
       "3                2.0                              NaN           75.0   \n",
       "4                2.0                              NaN           85.0   \n",
       "\n",
       "   tbi_ct_skull_fracture_characteristic  tbi_ct_subarachnoid_characteristic  \\\n",
       "0                                   NaN                                 NaN   \n",
       "1                                   NaN                                 NaN   \n",
       "2                                   2.0                                 NaN   \n",
       "3                                   2.0                                 NaN   \n",
       "4                                   2.0                                 NaN   \n",
       "\n",
       "   tbi_cli_hypertension  tbi_cli_temp  \n",
       "0                   2.0          36.5  \n",
       "1                   2.0          36.8  \n",
       "2                   2.0          36.6  \n",
       "3                   2.0          36.5  \n",
       "4                   2.0          36.8  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = [\"tbi_cli_glasgow\", \"ethanol\", \"tbi_cli_breathing_rate\", \"tbi_ct_subdural_hematoma_position_proprotion\", \"tbi_ct_subdural_hematoma_position_value\",\n",
    "\"tbi_ct_rotterdam\", \"tbi_ct_epidural_hematoma_value\", \"tbi_ct_subdural_hematoma_thick\", \"tbi_ct_cerebral_contusion_volume\",\n",
    "\"age_at_record\",\"tbi_cli_time_acci_hos\", \"tbi_ct_skull_risk\", \"tbi_ct_epidural_hematoma_volume\", \"tbi_cli_pulse\",\n",
    "\"tbi_ct_skull_fracture_characteristic\",\"tbi_ct_subarachnoid_characteristic\",\"tbi_cli_hypertension\",\"tbi_cli_temp\"]  # Replace with the names of the columns you want to select\n",
    "# Select the columns\n",
    "df_final = df_tbi[selected_columns]\n",
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a9a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260 entries, 0 to 259\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                        Non-Null Count  Dtype  \n",
      "---  ------                                        --------------  -----  \n",
      " 0   tbi_cli_glasgow                               260 non-null    int64  \n",
      " 1   ethanol                                       109 non-null    float64\n",
      " 2   tbi_cli_breathing_rate                        30 non-null     float64\n",
      " 3   tbi_ct_subdural_hematoma_position_proprotion  96 non-null     float64\n",
      " 4   tbi_ct_subdural_hematoma_position_value       79 non-null     float64\n",
      " 5   tbi_ct_rotterdam                              252 non-null    float64\n",
      " 6   tbi_ct_epidural_hematoma_value                40 non-null     float64\n",
      " 7   tbi_ct_subdural_hematoma_thick                93 non-null     float64\n",
      " 8   tbi_ct_cerebral_contusion_volume              78 non-null     float64\n",
      " 9   age_at_record                                 259 non-null    float64\n",
      " 10  tbi_cli_time_acci_hos                         240 non-null    float64\n",
      " 11  tbi_ct_skull_risk                             252 non-null    float64\n",
      " 12  tbi_ct_epidural_hematoma_volume               41 non-null     float64\n",
      " 13  tbi_cli_pulse                                 84 non-null     float64\n",
      " 14  tbi_ct_skull_fracture_characteristic          79 non-null     float64\n",
      " 15  tbi_ct_subarachnoid_characteristic            112 non-null    float64\n",
      " 16  tbi_cli_hypertension                          232 non-null    float64\n",
      " 17  tbi_cli_temp                                  84 non-null     float64\n",
      "dtypes: float64(17), int64(1)\n",
      "memory usage: 36.7 KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# df_sc = df_final\n",
    "df_target = df_tbi['d_kl_tl']\n",
    "# df_new_array = scaler.fit_transform(df_sc)\n",
    "# df_tbi_f = pd.DataFrame(df_new_array)\n",
    "# df_tbi_f\n",
    "features_to_scale = df_final.columns  \n",
    "scaled_features = scaler.fit_transform(df_final[features_to_scale])\n",
    "# Create a DataFrame with the scaled features\n",
    "df_tbi_f = pd.DataFrame(scaled_features, columns=features_to_scale)\n",
    "# Assuming you also want to include 'd_kl_tl' from df_tbi\n",
    "# df_tbi_f['d_kl_tl'] = df_tbi['d_kl_tl']\n",
    "df_final.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9c7ec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260 entries, 0 to 259\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                        Non-Null Count  Dtype  \n",
      "---  ------                                        --------------  -----  \n",
      " 0   tbi_cli_glasgow                               260 non-null    float64\n",
      " 1   ethanol                                       260 non-null    float64\n",
      " 2   tbi_cli_breathing_rate                        260 non-null    float64\n",
      " 3   tbi_ct_subdural_hematoma_position_proprotion  260 non-null    float64\n",
      " 4   tbi_ct_subdural_hematoma_position_value       260 non-null    float64\n",
      " 5   tbi_ct_rotterdam                              260 non-null    float64\n",
      " 6   tbi_ct_epidural_hematoma_value                260 non-null    float64\n",
      " 7   tbi_ct_subdural_hematoma_thick                260 non-null    float64\n",
      " 8   tbi_ct_cerebral_contusion_volume              260 non-null    float64\n",
      " 9   age_at_record                                 260 non-null    float64\n",
      " 10  tbi_cli_time_acci_hos                         260 non-null    float64\n",
      " 11  tbi_ct_skull_risk                             260 non-null    float64\n",
      " 12  tbi_ct_epidural_hematoma_volume               260 non-null    float64\n",
      " 13  tbi_cli_pulse                                 260 non-null    float64\n",
      " 14  tbi_ct_skull_fracture_characteristic          260 non-null    float64\n",
      " 15  tbi_ct_subarachnoid_characteristic            260 non-null    float64\n",
      " 16  tbi_cli_hypertension                          260 non-null    float64\n",
      " 17  tbi_cli_temp                                  260 non-null    float64\n",
      "dtypes: float64(18)\n",
      "memory usage: 36.7 KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_tbi_i = pd.DataFrame(knn_imputer.fit_transform(df_tbi_f), columns=df_tbi_f.columns)\n",
    "df_tbi_i.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49d3d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df_tbi_i\n",
    "y = df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3342436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    142\n",
      "3     80\n",
      "1     20\n",
      "4     18\n",
      "Name: d_kl_tl, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78302e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194f9a2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8c1d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df_tbi_i\n",
    "y = df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88fab591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "svm_model = make_pipeline(SVC())\n",
    "logreg_model = make_pipeline(LogisticRegression())\n",
    "dt_model = make_pipeline(DecisionTreeClassifier())\n",
    "knn_model = make_pipeline(KNeighborsClassifier())\n",
    "gnb = make_pipeline(GaussianNB())\n",
    "rf = make_pipeline(RandomForestClassifier())\n",
    "\n",
    "# Create a list of models\n",
    "models = [logreg_model,svm_model,  dt_model, knn_model, gnb,rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3eb5a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and their corresponding parameter grids\n",
    "svm_model = make_pipeline(SVC())\n",
    "svm_param_grid = {'svc__kernel': ['linear', 'rbf']}\n",
    "\n",
    "logreg_model = make_pipeline(LogisticRegression(max_iter=1000))\n",
    "logreg_param_grid = {'logisticregression__C': [0.1, 1, 10]}\n",
    "\n",
    "dt_model = make_pipeline(DecisionTreeClassifier())\n",
    "dt_param_grid = {'decisiontreeclassifier__max_depth': [None, 10, 20], 'decisiontreeclassifier__min_samples_split': [2, 5, 10]}\n",
    "\n",
    "knn_model = make_pipeline(KNeighborsClassifier())\n",
    "knn_param_grid = {'kneighborsclassifier__n_neighbors': [3, 5, 7]}\n",
    "\n",
    "gnb_model = make_pipeline(GaussianNB())\n",
    "\n",
    "rf_model = make_pipeline(RandomForestClassifier())\n",
    "rf_param_grid = {'randomforestclassifier__n_estimators': [100, 200,300],\n",
    "                 'randomforestclassifier__max_depth': [None, 10],\n",
    "                 'randomforestclassifier__min_samples_split': [5, 10],\n",
    "                 'randomforestclassifier__min_samples_leaf': [2, 4]}\n",
    "\n",
    "# Create a list of models with their corresponding parameter grids\n",
    "models = [\n",
    "    (logreg_model, logreg_param_grid),\n",
    "    (svm_model, svm_param_grid),\n",
    "    (dt_model, dt_param_grid),\n",
    "    (knn_model, knn_param_grid),\n",
    "    (gnb_model, {}),\n",
    "    (rf_model, rf_param_grid)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ea549",
   "metadata": {},
   "source": [
    "#### LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "sensitivity_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "# Use leave-one-out cross-validation for evaluation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Iterate over models\n",
    "for model, param_grid in models:\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=loo, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Get best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train model on training data\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on test data\n",
    "        y_pred.extend(best_model.predict(X_test))\n",
    "        y_true.extend(y_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')  # Specify average='weighted' for multiclass\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate sensitivity and specificity for each class\n",
    "    sensitivity = []\n",
    "    specificity = []\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        tp = conf_matrix[i, i]\n",
    "        fn = np.sum(conf_matrix[i, :]) - tp\n",
    "        fp = np.sum(conf_matrix[:, i]) - tp\n",
    "        tn = np.sum(conf_matrix) - tp - fn - fp\n",
    "        \n",
    "        sensitivity.append(tp / (tp + fn))\n",
    "        specificity.append(tn / (tn + fp))\n",
    "    \n",
    "    # Append mean sensitivity and specificity to lists\n",
    "    sensitivity_scores.append(np.mean(sensitivity))\n",
    "    specificity_scores.append(np.mean(specificity))\n",
    "    \n",
    "    # Append results to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Print results\n",
    "for i, (model, _) in enumerate(models):\n",
    "    print(f\"Model {i+1}: {list(model.named_steps.keys())} - Accuracy: {accuracy_scores[i]}, F1 Score: {f1_scores[i]}\")\n",
    "    print(f\"Mean Sensitivity (Recall): {sensitivity_scores[i]}, Mean Specificity: {specificity_scores[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfba13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "sensitivity_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "# Use leave-one-out cross-validation for evaluation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Iterate over models\n",
    "for model, param_grid in models:\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=loo, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Get best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train model on training data\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on test data\n",
    "        y_pred.extend(best_model.predict(X_test))\n",
    "        y_true.extend(y_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')  # Specify average='weighted' for multiclass\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate sensitivity and specificity for each class\n",
    "    sensitivity = []\n",
    "    specificity = []\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        tp = conf_matrix[i, i]\n",
    "        fn = np.sum(conf_matrix[i, :]) - tp\n",
    "        fp = np.sum(conf_matrix[:, i]) - tp\n",
    "        tn = np.sum(conf_matrix) - tp - fn - fp\n",
    "        \n",
    "        sensitivity.append(tp / (tp + fn))\n",
    "        specificity.append(tn / (tn + fp))\n",
    "    \n",
    "    # Append mean sensitivity and specificity to lists\n",
    "    sensitivity_scores.append(np.mean(sensitivity))\n",
    "    specificity_scores.append(np.mean(specificity))\n",
    "    \n",
    "    # Append results to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Print results\n",
    "for i, (model, _) in enumerate(models):\n",
    "    print(f\"Model {i+1}: {list(model.named_steps.keys())} - Accuracy: {accuracy_scores[i]}, F1 Score: {f1_scores[i]}\")\n",
    "    print(f\"Mean Sensitivity (Recall): {sensitivity_scores[i]}, Mean Specificity: {specificity_scores[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc263aaf",
   "metadata": {},
   "source": [
    "#### K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb618e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define 10-fold cross-validation\n",
    "k_fold = KFold(n_splits=10)\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "for model, param_grid in models:\n",
    "    model_name = model.steps[-1][0]  # Extract the name of the classifier\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid, cv=k_fold, scoring='accuracy')\n",
    "    grid_search.fit( X, y)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(f'Best Parameters ({model_name}): {best_params}')\n",
    "    print(f'Best Accuracy ({model_name}): {best_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "sensitivity_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "# Use leave-one-out cross-validation for evaluation\n",
    "k_fold = KFold(n_splits=10)\n",
    "\n",
    "# Iterate over models\n",
    "for model, param_grid in models:\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=k_fold, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Get best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for train_index, test_index in k_fold.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train model on training data\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on test data\n",
    "        y_pred.extend(best_model.predict(X_test))\n",
    "        y_true.extend(y_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')  # Specify average='weighted' for multiclass\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate sensitivity and specificity for each class\n",
    "    sensitivity = []\n",
    "    specificity = []\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        tp = conf_matrix[i, i]\n",
    "        fn = np.sum(conf_matrix[i, :]) - tp\n",
    "        fp = np.sum(conf_matrix[:, i]) - tp\n",
    "        tn = np.sum(conf_matrix) - tp - fn - fp\n",
    "        \n",
    "        sensitivity.append(tp / (tp + fn))\n",
    "        specificity.append(tn / (tn + fp))\n",
    "    \n",
    "    # Append mean sensitivity and specificity to lists\n",
    "    sensitivity_scores.append(np.mean(sensitivity))\n",
    "    specificity_scores.append(np.mean(specificity))\n",
    "    \n",
    "    # Append results to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Print results\n",
    "for i, (model, _) in enumerate(models):\n",
    "    print(f\"Model {i+1}: {list(model.named_steps.keys())} - Accuracy: {accuracy_scores[i]}, F1 Score: {f1_scores[i]}\")\n",
    "    print(f\"Mean Sensitivity (Recall): {sensitivity_scores[i]}, Mean Specificity: {specificity_scores[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca269ed1",
   "metadata": {},
   "source": [
    "### SMOTE + GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46daaf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# Apply SMOTE to address class imbalance\n",
    "smote = SMOTE(random_state= None)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "print(f'''Shape of X before SMOTE: {X.shape}\n",
    "Shape of X after SMOTE: {X_resampled.shape}''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a2bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6882e6",
   "metadata": {},
   "source": [
    "#### LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bb514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "for model, param_grid in models:\n",
    "    model_name = model.steps[-1][0]  # Extract the name of the classifier\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid, cv=loo, scoring='accuracy')\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(f'Best Parameters ({model_name}): {best_params}')\n",
    "    print(f'Best Accuracy ({model_name}): {best_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad7622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store results\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "sensitivity_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "# Use leave-one-out cross-validation for evaluation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Iterate over models\n",
    "for model, param_grid in models:\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=loo, scoring='accuracy')\n",
    "    grid_search.fit(X, y)  # Use full dataset for grid search\n",
    "    \n",
    "    # Get best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Apply SMOTE on training data\n",
    "        smote = SMOTE(random_state=None)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Train model on resampled training data\n",
    "        best_model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Predict on test data\n",
    "        y_pred.extend(best_model.predict(X_test))\n",
    "        y_true.extend(y_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')  # Specify average='weighted' for multiclass\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate sensitivity and specificity for each class\n",
    "    sensitivity = []\n",
    "    specificity = []\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        tp = conf_matrix[i, i]\n",
    "        fn = np.sum(conf_matrix[i, :]) - tp\n",
    "        fp = np.sum(conf_matrix[:, i]) - tp\n",
    "        tn = np.sum(conf_matrix) - tp - fn - fp\n",
    "        \n",
    "        sensitivity.append(tp / (tp + fn))\n",
    "        specificity.append(tn / (tn + fp))\n",
    "    \n",
    "    # Append mean sensitivity and specificity to lists\n",
    "    sensitivity_scores.append(np.mean(sensitivity))\n",
    "    specificity_scores.append(np.mean(specificity))\n",
    "    \n",
    "    # Append results to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Print results\n",
    "for i, (model, _) in enumerate(models):\n",
    "    print(f\"Model {i+1}: {list(model.named_steps.keys())} - Accuracy: {accuracy_scores[i]}, F1 Score: {f1_scores[i]}\")\n",
    "    print(f\"Mean Sensitivity (Recall): {sensitivity_scores[i]}, Mean Specificity: {specificity_scores[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "sensitivity_scores = []\n",
    "specificity_scores = []\n",
    "conf_matrices = []\n",
    "\n",
    "# Use leave-one-out cross-validation for evaluation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Iterate over models\n",
    "for model, param_grid in models:\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=loo, scoring='accuracy')\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Get best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for train_index, test_index in loo.split(X,y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index],y.iloc[test_index]\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Train model on training data\n",
    "        best_model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Predict on test data\n",
    "        y_pred.extend(best_model.predict(X_test))\n",
    "        y_true.extend(y_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')  # Specify average='weighted' for multiclass\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate sensitivity and specificity for each class\n",
    "    sensitivity = []\n",
    "    specificity = []\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        tp = conf_matrix[i, i]\n",
    "        fn = np.sum(conf_matrix[i, :]) - tp\n",
    "        fp = np.sum(conf_matrix[:, i]) - tp\n",
    "        tn = np.sum(conf_matrix) - tp - fn - fp\n",
    "        \n",
    "        sensitivity.append(tp / (tp + fn))\n",
    "        specificity.append(tn / (tn + fp))\n",
    "    \n",
    "    # Append mean sensitivity and specificity to lists\n",
    "    sensitivity_scores.append(np.mean(sensitivity))\n",
    "    specificity_scores.append(np.mean(specificity))\n",
    "    \n",
    "    # Append results to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Print results\n",
    "for i, (model, _) in enumerate(models):\n",
    "    print(f\"Model {i+1}: {list(model.named_steps.keys())} - Accuracy: {accuracy_scores[i]}, F1 Score: {f1_scores[i]}\")\n",
    "    print(f\"Mean Sensitivity (Recall): {sensitivity_scores[i]}, Mean Specificity: {specificity_scores[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733cb618",
   "metadata": {},
   "source": [
    "#### k fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "sensitivity_scores = []\n",
    "specificity_scores = []\n",
    "conf_matrices = []\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "k_fold = KFold(n_splits=10)\n",
    "\n",
    "# Iterate over models\n",
    "for model, param_grid in models:\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=k_fold, scoring='accuracy')\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Get best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for train_index, test_index in k_fold.split(X_resampled):\n",
    "        X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "        y_train, y_test = y_resampled.iloc[train_index],y_resampled.iloc[test_index]\n",
    "        \n",
    "        # Train model on training data\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on test data\n",
    "        y_pred.extend(best_model.predict(X_test))\n",
    "        y_true.extend(y_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')  # Specify average='weighted' for multiclass\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate sensitivity and specificity for each class\n",
    "    sensitivity = []\n",
    "    specificity = []\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        tp = conf_matrix[i, i]\n",
    "        fn = np.sum(conf_matrix[i, :]) - tp\n",
    "        fp = np.sum(conf_matrix[:, i]) - tp\n",
    "        tn = np.sum(conf_matrix) - tp - fn - fp\n",
    "        \n",
    "        sensitivity.append(tp / (tp + fn))\n",
    "        specificity.append(tn / (tn + fp))\n",
    "    \n",
    "    # Append mean sensitivity and specificity to lists\n",
    "    sensitivity_scores.append(np.mean(sensitivity))\n",
    "    specificity_scores.append(np.mean(specificity))\n",
    "    \n",
    "    # Append results to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Print results\n",
    "for i, (model, _) in enumerate(models):\n",
    "    print(f\"Model {i+1}: {list(model.named_steps.keys())} - Accuracy: {accuracy_scores[i]}, F1 Score: {f1_scores[i]}\")\n",
    "    print(f\"Mean Sensitivity (Recall): {sensitivity_scores[i]}, Mean Specificity: {specificity_scores[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3da91",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b114ef8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_resampled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Perform grid search with cross-validation\u001b[39;00m\n\u001b[0;32m     34\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mrf_classifier, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mscoring, refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_resampled\u001b[49m, y_resampled)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Get the best estimator\u001b[39;00m\n\u001b[0;32m     38\u001b[0m best_rf_classifier \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_resampled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Define a custom scoring function to compute sensitivity and specificity\n",
    "def sensitivity_specificity_scoring(y_true, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# Create a scoring dictionary with multiple metrics\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_score': make_scorer(f1_score, average='weighted'),\n",
    "    'sensitivity': make_scorer(sensitivity_specificity_scoring, greater_is_better=True),\n",
    "    'specificity': make_scorer(sensitivity_specificity_scoring, greater_is_better=True)\n",
    "}\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=10, scoring=scoring, refit='accuracy')\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Get the best estimator\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Get the results\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "# Print the best parameters and scores\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy Score:\", grid_search.best_score_)\n",
    "print(\"Best F1 Score:\", cv_results['mean_test_f1_score'][grid_search.best_index_])\n",
    "print(\"Best Sensitivity:\", cv_results['mean_test_sensitivity'][grid_search.best_index_])\n",
    "print(\"Best Specificity:\", cv_results['mean_test_specificity'][grid_search.best_index_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "594d5a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.8908450704225352\n",
      "F1 Score: 0.8893980132589218\n",
      "Sensitivity: 0.8908450704225352\n",
      "Specificity: [0.97183099 0.79577465 0.79577465 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create a model (Random Forest Classifier)\n",
    "model = RandomForestClassifier(max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200)\n",
    "\n",
    "# Perform Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "y_pred = cross_val_predict(model, X_resampled, y_resampled, cv=loo)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_resampled, y_pred)\n",
    "print(\"Mean Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_resampled, y_pred, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = recall_score(y_resampled, y_pred, average='weighted')\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "# Since specificity is not directly supported by sklearn, we need to calculate it manually\n",
    "# Specificity = TN / (TN + FP)\n",
    "# Where TN is True Negative and FP is False Positive\n",
    "conf_matrix = confusion_matrix(y_resampled, y_pred)\n",
    "specificity = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "print(\"Specificity:\", specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "339489e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6884615384615385\n",
      "F1 Score: 0.6586470556751238\n",
      "Sensitivity: 0.6884615384615385\n",
      "Specificity: [0.05       0.87323944 0.5        0.77777778]\n"
     ]
    }
   ],
   "source": [
    "# Create a model (Random Forest Classifier)\n",
    "model = RandomForestClassifier(max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100)\n",
    "\n",
    "# Perform Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "y_pred = cross_val_predict(model, X,y, cv=loo)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Mean Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y, y_pred, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = recall_score(y, y_pred, average='weighted')\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "# Since specificity is not directly supported by sklearn, we need to calculate it manually\n",
    "# Specificity = TN / (TN + FP)\n",
    "# Where TN is True Negative and FP is False Positive\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "specificity = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "print(\"Specificity:\", specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773ef0c",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb27fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# Define the models\n",
    "svm_model = make_pipeline(SVC())\n",
    "logreg_model = make_pipeline(LogisticRegression())\n",
    "dt_model = make_pipeline(DecisionTreeClassifier())\n",
    "knn_model = make_pipeline(KNeighborsClassifier(n_neighbors=5))\n",
    "gnb = make_pipeline(GaussianNB())\n",
    "\n",
    "# Create a list of models\n",
    "models = [(\"SVM\", svm_model), (\"Logistic Regression\", logreg_model), \n",
    "          (\"Decision Tree\", dt_model), (\"KNN\", knn_model), (\"Gaussian NB\", gnb)]\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models:\n",
    "    print(f\"Evaluating {name} model:\")\n",
    "    \n",
    "    # Calculate cross-validated accuracy\n",
    "    accuracy = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    print(f\"Accuracy: {accuracy.mean():.4f} ({accuracy.std():.2f})\")\n",
    "    \n",
    "    # Calculate cross-validated F1 score\n",
    "    f1 = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n",
    "    print(f\"F1 Score: {f1.mean():.4f} ({f1.std():.2f})\")\n",
    "    \n",
    "    # Calculate cross-validated recall\n",
    "    recall = cross_val_score(model, X, y, cv=5, scoring='recall_macro')\n",
    "    print(f\"Recall: {recall.mean():.4f} ({recall.std():.2f})\")\n",
    "    \n",
    "    # Calculate cross-validated precision\n",
    "    precision = cross_val_score(model, X, y, cv=5, scoring='precision_macro')\n",
    "    print(f\"Precision: {precision.mean():.4f} ({precision.std():.2f})\")\n",
    "    \n",
    "    print()  # Add empty line for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define the models\n",
    "svm_model = make_pipeline(SVC())\n",
    "logreg_model = make_pipeline(LogisticRegression())\n",
    "dt_model = make_pipeline(DecisionTreeClassifier())\n",
    "knn_model = make_pipeline(KNeighborsClassifier(n_neighbors=5))\n",
    "gnb_model = make_pipeline(GaussianNB())\n",
    "\n",
    "# Create a list of models\n",
    "models = [(\"SVM\", svm_model), (\"Logistic Regression\", logreg_model), \n",
    "          (\"Decision Tree\", dt_model), (\"KNN\", knn_model), (\"Gaussian NB\", gnb_model)]\n",
    "\n",
    "# Perform train/test split with 80/20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models:\n",
    "    print(f\"Evaluating {name} model:\")\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    precision = precision_score(y_test, y_pred, average='macro',zero_division=1)\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print()  # Add empty line for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54061fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming svm_model is your trained SVM model\n",
    "# Make predictions on the test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "# Calculate evaluation metrics\n",
    "print(y_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro',zero_division=1)\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.6f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print()  # Add empty line for better readability\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Define class labels\n",
    "classes = ['Mild', 'Moderate', 'Severe', 'Very Severe']\n",
    "\n",
    "# Create a DataFrame from the confusion matrix\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')\n",
    "plt.title('Confusion Matrix - SVM Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb0c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
